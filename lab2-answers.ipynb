{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AI in Fact and Fiction - Summer 2021</h1>\n",
    "<h2>Introduction to Computer Vision</h2>\n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this lab, you will learn the basics of Computer Vision, how to apply Convolutional Neural Networks, and how to tweak them.</p>\n",
    "\n",
    "* Use [Google Colab](https://colab.research.google.com/github/AIFictionFact/Summer2021/blob/master/lab2.ipynb) to run the python code, and to complete any missing lines of code.\n",
    "* You might find it helpful to save this notebook on your Google Drive.\n",
    "* Please make sure to fill the required information in the **Declaration** cell.\n",
    "* Once you complete the lab, please download the .ipynb file (File --> Download .ipynb).\n",
    "* Then, please use the following file naming convention to rename the downloaded python file lab2_YourRCS.ipynb (make sure to replace 'YourRCS' with your RCS ID, for example 'lab2_senevo.ipynb').\n",
    "* Submit the .ipynb file in LMS.\n",
    "\n",
    "<p>Due Date/Time: <b>Friday, Jul 2 1.00 PM ET</b></p>\n",
    "\n",
    "<p>Estimated Time Needed: <b>4 hours</b></p>\n",
    "\n",
    "<p>Total Tasks: <b>8</b></p>\n",
    "<p>Total Points: <b>50</b></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "source": [
    "\n",
    "**Declaration**\n",
    "\n",
    "*Your Name* :\n",
    "\n",
    "*Collaborators (if any)* :\n",
    "\n",
    "*Online Resources consulted (if any):*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 1 - Recognizing Handwritten Digits\n",
    "\n",
    "In this section, you will use a single layer neural network to classify handwritten digits from the MNIST database, that contains handwritten digit images. For more information about MNIST please see http://yann.lecun.com/exdb/mnist.\n",
    "\n",
    "We'll need the following libraries. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following line to install the torchvision library, if it is not already installed\n",
    "#!conda install -y torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import normal\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "source": [
    "Use the following function to plot the parameters of the neural network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameters(model): \n",
    "    W = model.state_dict()['linear.weight'].data\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(2, 5)\n",
    "    fig.subplots_adjust(hspace=0.01, wspace=0.1)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < 10:\n",
    "            \n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"class: {0}\".format(i))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :].view(28, 28), vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        # Ensure the plot is shown correctly with multiple plots\n",
    "        # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "Use the following function to visualize the data: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1]))"
   ]
  },
  {
   "source": [
    "<h3 id=\"Makeup_Data\">Make Some Data</h3> \n",
    "\n",
    "Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code> (Do you remember <a href=\"https://pytorch.org/docs/stable/torchvision/transforms.html\">transforms</a> from Lab 1? ;))."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1122)>\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error downloading train-images-idx3-ubyte.gz",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2b0fdfb0105e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create and print the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Print the training dataset:\\n \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error downloading {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error downloading train-images-idx3-ubyte.gz"
     ]
    }
   ],
   "source": [
    "# Create and print the training dataset\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the training dataset:\\n \", train_dataset)"
   ]
  },
  {
   "source": [
    "Load the testing dataset by setting the parameters <code>train</code> to <code>False</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print the validating dataset\n",
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the validating dataset:\\n \", validation_dataset)"
   ]
  },
  {
   "source": [
    "You can see that the data type is an integer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of the element\n",
    "print(\"Type of data element: \", type(train_dataset[0][1]))"
   ]
  },
  {
   "source": [
    "Each element in the rectangular tensor corresponds to a number that represents a pixel intensity as demonstrated by the following image:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/MNIST_1_image_values.png\" width=\"550\" alt=\"MNIST elements\" />\n",
    "\n",
    "In this image, the values are inverted, i.e., black represents white.\n",
    "\n",
    "Print out the label of the fourth sample:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the label\n",
    "print(\"The label: \", train_dataset[3][1])"
   ]
  },
  {
   "source": [
    "The result shows the number in the image is 1.\n",
    "\n",
    "Plot the fourth sample:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image\n",
    "print(\"The image: \", show_data(train_dataset[3]))"
   ]
  },
  {
   "source": [
    "You see that it is a 1. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3 id=\"#Classifier\">Build a Softmax Classifer</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Prediction\"\"\"\n",
    "        z = self.linear(x)\n",
    "        return z"
   ]
  },
  {
   "source": [
    "The Softmax function requires vector inputs. Note that the vector shape is 28x28."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of train dataset\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "source": [
    "Flatten the tensor as shown in this image: \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/Imagetovector2.png\" width=\"550\" alt=\"Flattern Image\" />\n",
    "\n",
    "The size of the input tensor is 784, and the output tensor is 10 (because it is recognizing the digits from 0-9.)\n",
    "\n",
    "Set the input size and output size: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input size and output size\n",
    "input_dim = 28 * 28\n",
    "output_dim = 10"
   ]
  },
  {
   "source": [
    "Define the Softmax Classifier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = SoftMax(input_dim, output_dim)\n",
    "print(\"Print the model:\\n \", model)"
   ]
  },
  {
   "source": [
    "View the size of the model parameters: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters\n",
    "print('W: ',list(model.parameters())[0].size())\n",
    "print('b: ',list(model.parameters())[1].size())"
   ]
  },
  {
   "source": [
    "You can convert the model parameters for each class (0-9) to a rectangular grid: \n",
    "\n",
    "<a><img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/paramaters_to_image.gif\" width = 550, align = \"center\"></a> \n",
    "\n",
    "Plot the model parameters for each class as a square image: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model parameters for each class\n",
    "plot_parameters(model)"
   ]
  },
  {
   "source": [
    "Define the learning rate, optimizer, criterion (i.e., the loss funtion), and the data loader:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate, optimizer, criterion and data loader\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "source": [
    "Train the model and determine validation accuracy **(should take a few minutes)**: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_epochs = 10\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "n_test = len(validation_dataset)\n",
    "\n",
    "def train_model(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        correct = 0\n",
    "        # perform a prediction on the validationdata  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            z = model(x_test.view(-1, 28 * 28))\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / n_test\n",
    "        loss_list.append(loss.data)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "train_model(n_epochs)"
   ]
  },
  {
   "source": [
    "<h3 id=\"Result\">Analyze Results</h3> \n",
    "Plot the loss and accuracy on the validation data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(loss_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "source": [
    "View the results of the parameters for each class after the training. You can see that they look like the corresponding numbers. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameters\n",
    "plot_parameters(model)"
   ]
  },
  {
   "source": [
    "Let's plot the first five misclassified  samples and the probability of that class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the misclassified samples\n",
    "Softmax_fn=nn.Softmax(dim=-1)\n",
    "count = 0\n",
    "for x, y in validation_dataset:\n",
    "    z = model(x.reshape(-1, 28 * 28))\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    if yhat != y:\n",
    "        show_data((x, y))\n",
    "        plt.show()\n",
    "        print(\"yhat:\", yhat)\n",
    "        print(\"probability of class \", torch.max(Softmax_fn(z)).item())\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break       "
   ]
  },
  {
   "source": [
    "Let's plot the first five correctly classified samples and the probability of that class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the classified samples\n",
    "Softmax_fn=nn.Softmax(dim=-1)\n",
    "count = 0\n",
    "for x, y in validation_dataset:\n",
    "    z = model(x.reshape(-1, 28 * 28))\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    if yhat == y:\n",
    "        show_data((x, y))\n",
    "        plt.show()\n",
    "        print(\"yhat:\", yhat)\n",
    "        print(\"probability of class \", torch.max(Softmax_fn(z)).item())\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        break  "
   ]
  },
  {
   "source": [
    "<h3 id=\"task-01\">Task 01 (5 points)</h3>\n",
    "\n",
    "__(This is a written question)__\n",
    "\n",
    "What is your observation on the misclassified and correctly classified digits from the MNIST dataset? You may want to check the misclassified and correctly classified samples and their classification probabilities beyond the first five used in the lab. (max 100 words)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_Please type your answer to Task 01 here._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 2 - Convolutional Neural Networks\n",
    "\n",
    "In this part of the lab, you will study convolution and review how the different operations change the relationship between input and output.\n",
    "\n",
    "### What is Convolution?\n",
    "\n",
    "Convolution is a linear operation similar to a linear equation, dot product, or matrix multiplication. Convolution has several advantages for analyzing images as it preserves the relationship between elements, and it requires fewer parameters than other methods. \n",
    "\n",
    "You can see the relationship between the different methods that you learned:\n",
    "\n",
    "$$linear \\ equation :y=wx+b$$\n",
    "$$linear\\ equation\\ with\\ multiple \\ variables \\ where \\ \\mathbf{x} \\ is \\ a \\ vector : \\mathbf{y}=\\mathbf{wx}+b$$\n",
    "$$ \\ matrix\\ multiplication \\ where \\ \\mathbf{X} \\ is \\ a \\ matrix : \\mathbf{y}=\\mathbf{wX}+\\mathbf{b} $$\n",
    "$$\\ convolution \\ where \\ \\mathbf{X} \\ and \\ \\mathbf{Y} \\ are \\ tensors :  \\mathbf{Y}=\\mathbf{w}*\\mathbf{X}+\\mathbf{b}$$\n",
    "\n",
    "Create a two-dimensional convolution object by using the constructor <code>nn.Conv2d</code>, with the parameters <code>in_channels</code>, <code>out_channels</code>, and the parameter <code>kernel_size</code> (which will be set to 3)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "source": [
    "Because the weight and bias parameters in <code>nn.Conv2d</code> are randomly initialized and learned through training, give them some initial values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n",
    "conv.state_dict()['bias'][0]=0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "source": [
    "Create a dummy tensor to represent an image. The parameters of this dummy tensor correspond to the following:\n",
    "\n",
    "(number of inputs, number of channels, number of rows, number of columns) \n",
    "\n",
    "Then, set the third column to 1:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=torch.zeros(1,1,5,5)\n",
    "image[0,0,:,2]=1\n",
    "image"
   ]
  },
  {
   "source": [
    "Call the object <code>conv</code> on the tensor <code>image</code> as an input to perform the convolution and assign the result to the tensor <code>z</code>. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=conv(image)\n",
    "z"
   ]
  },
  {
   "source": [
    "The following animation illustrates the process. The kernel performs at the element-level multiplication on every element in the image in the corresponding region. The values are then added together. The kernel is then shifted and the process is repeated. \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/convolution.gif\" width = 500, align = \"center\">\n",
    "\n",
    "<h3>Determining  the Size of the Output</h3>\n",
    "\n",
    "The size of the output is an important parameter. In this lab, you will assume square images. For rectangular images, the same formula can be used for each dimension independently.  \n",
    "\n",
    "Let M be the size of the input and K be the size of the kernel. The size of the output is given by the following formula:\n",
    "$$M_{new}=M-K+1$$\n",
    "\n",
    "Let's create a kernel of size 2:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=2\n",
    "conv_k2 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K)\n",
    "conv_k2.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv_k2.state_dict()['bias'][0]=0.0\n",
    "conv_k2.state_dict()\n",
    "conv_k2"
   ]
  },
  {
   "source": [
    "Create an image of size 4:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4\n",
    "image1=torch.ones(1,1,M,M)"
   ]
  },
  {
   "source": [
    "The image and the kernel are as follows:\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/kernal2.png\" width = 500, align = \"center\">\n",
    "\n",
    "The following equation provides the output:\n",
    "\n",
    "$$M_{out}=M-K+1$$\n",
    "$$M_{out}=4-2+1$$\n",
    "$$M_{out}=3$$\n",
    "\n",
    "The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. As the kernel is of size K, there are M-K  elements for the kernel to move in the horizontal direction. The same logic applies to the vertical direction.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/outsize.gif\" width = 500, align = \"center\">\n",
    "\n",
    "Perform the convolution and verify the size is correct:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_k2=conv_k2(image1)\n",
    "print(\"z_k2:\",z_k2)\n",
    "print(\"shape:\",z_k2.shape[2:4])"
   ]
  },
  {
   "source": [
    "<h3>Stride parameter</h3>\n",
    "\n",
    "The parameter stride changes the number of shifts the kernel moves per iteration. As a result, the output size also changes and is given by the following formula:\n",
    "\n",
    "$$M_{out}=\\dfrac{M-K}{stride}+1$$\n",
    "\n",
    "Create a convolution object with a stride of 2:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_s2 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)\n",
    "\n",
    "conv_s2.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv_s2.state_dict()['bias'][0]=0.0\n",
    "conv_s2.state_dict()"
   ]
  },
  {
   "source": [
    "For an image with a size of 4, here is how we calculate the output size:\n",
    "\n",
    "$$M_{out}=\\dfrac{M-K}{stride}+1$$\n",
    "$$M_{out}=\\dfrac{4-2}{2}+1$$\n",
    "$$M_{out}=2$$\n",
    "\n",
    "The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. Because the kernel is of size K, there are M-K=2 elements. The stride is 2 because it will move 2 elements at a time. As a result, you divide M-K by the stride value 2:\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/stride2.gif\" width = 500, align = \"center\">\n",
    "\n",
    "Perform the convolution and verify the size is correct: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_s2=conv_s2(image1)\n",
    "\n",
    "print(\"z_s2:\",z_s2)\n",
    "print(\"shape:\",z_s2.shape[2:4])"
   ]
  },
  {
   "source": [
    "<h3>Zero Padding </h3>\n",
    "As you apply successive convolutions, the image will shrink. You can apply zero padding to keep the image at a reasonable size, which also holds information at the borders.\n",
    "In addition, you might not get integer values for the size of the kernel. Consider the following image:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1"
   ]
  },
  {
   "source": [
    "Try performing convolutions with the <code>kernel_size=2</code> and a <code>stride=3</code>.\n",
    "\n",
    "$$M_{out}=\\dfrac{M-K}{stride}+1$$\n",
    "$$M_{out}=\\dfrac{4-2}{3}+1$$\n",
    "$$M_{out}=1.666$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_k2_s3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "conv_k2_s3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv_k2_s3.state_dict()['bias'][0]=0.0\n",
    "conv_k2_s3.state_dict()\n",
    "z_k2_s3=conv_k2_s3(image1)\n",
    "print(\"z_k2_s3:\",z_k2_s3)\n",
    "print(\"z_k2_s3:\",z_k2_s3.shape[2:4])"
   ]
  },
  {
   "source": [
    "You can add rows and columns of zeros around the image. This is called padding. In the constructor <code>Conv2d</code>, you specify the number of rows or columns of zeros that you want to add with the parameter padding. \n",
    "\n",
    "For a square image, you merely pad an extra column of zeros to the first column and the last column. Repeat the process for the rows. As a result, for a square image, the width and height is the original size plus 2 x the number of padding elements specified. You can then determine the size of the output after subsequent operations accordingly as shown in the following equation where you determine the size of an image after padding and then applying a convolutions kernel of size K.\n",
    "\n",
    "$$M'=M+2 \\times padding$$\n",
    "$$M_{out}=M'-K+1$$\n",
    "\n",
    "Consider the following example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_k2_s3_p1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)\n",
    "\n",
    "conv_k2_s3_p1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv_k2_s3_p1.state_dict()['bias'][0]=0.0\n",
    "conv_k2_s3_p1.state_dict()\n",
    "z_k2_s3_p1=conv_k2_s3_p1(image1)\n",
    "print(\"z with k=2 s=3 p=1:\",z_k2_s3_p1)\n",
    "print(\"z with k=2 s=3 p=1:\",z_k2_s3_p1.shape[2:4])"
   ]
  },
  {
   "source": [
    "The process is summarized in the following  animation: \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/zeropad.gif\" width = 500, align = \"center\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task 02 - Apply convolution (4 points)\n",
    "\n",
    "Create a random image of size 4 using <code>torch.randn</code>. Define kernel of zeros with a kernel size=3 (please initialize the weight and bias of the kernel convolution object to all zeros). Apply the kernel you defined to the random image you created. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to define a random image of size 4 (1 point)\n",
    "\n",
    "# Type your code here to define a kernel of zeros with a kernel size=3 (2 points)\n",
    "\n",
    "# Type your code here to perform the convolution (1 point)"
   ]
  },
  {
   "source": [
    "### Activation Functions\n",
    "\n",
    "Just like a neural network, you apply an activation function to the \"activation map\" obtain through the convolution explained above as shown in the following image:\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/block_digram.png\" width = 1000, align = \"center\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Create a kernel. Set the weight of the convolution object to the kernal and bias to zero: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "kernal=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0,-1.0]])\n",
    "conv.state_dict()['weight'][0][0]=kernal\n",
    "conv.state_dict()['bias'][0]=0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "source": [
    "Create an image."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=torch.zeros(1,1,5,5)\n",
    "image[0,0,:,2]=1\n",
    "image"
   ]
  },
  {
   "source": [
    "The following depicts the image and kernel we just created: \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/kernal_out.png\" width = 500, align = \"center\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Apply convolution to the image: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = conv(image)\n",
    "z"
   ]
  },
  {
   "source": [
    "Apply the activation function to the activation map. This will apply the activation function to each element in the activation map."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "a = relu(z)"
   ]
  },
  {
   "source": [
    "The process is summarized in the the following figure. The Relu function is applied to each element. All the elements less than zero are mapped to zero. The remaining components do not change.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/block_example.gif\" width = 1000, align = \"center\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Max pooling\n",
    "\n",
    "Max pooling reduces the number of parameters and makes the network less susceptible to changes in the image. \n",
    "Consider the following image: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=torch.zeros(1,1,4,4)\n",
    "image1[0,0,0,:]=torch.tensor([1.0,2.0,3.0,-4.0])\n",
    "image1[0,0,1,:]=torch.tensor([0.0,2.0,-3.0,0.0])\n",
    "image1[0,0,2,:]=torch.tensor([0.0,2.0,3.0,1.0])\n",
    "\n",
    "image1"
   ]
  },
  {
   "source": [
    "Max pooling simply takes the maximum value in each region. Consider the following image. For the first region, max pooling simply takes the largest element in a yellow region.   \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/maxpool_1.png\" width = 500, align = \"center\">\n",
    "\n",
    "If the stride is set to None (its defaults setting), the process will simply take the maximum in a prescribed area and shift over accordingly as shown in the following figure:\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/max_pool_animation_2.gif\" width = 500, align = \"center\">\n",
    "\n",
    "Here's the code in Pytorch:  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool=torch.nn.MaxPool2d(2) # kernel size = 2\n",
    "max_pool(image1)"
   ]
  },
  {
   "source": [
    "## Task 03 - Apply maxpooling (3 points)\n",
    "\n",
    "Task 03-1: Create a maxpooling object in 2d to perform max pooling with stride = 1 on <code>image1</code>. (1 point)\n",
    "\n",
    "Task 03-2: Provide a written answer to the following question (max 200 words).\n",
    "How does output size change when the stride = 1 compared to the default stride value? What is the reason for the change in the output size (you may explain your answer with the equations introduced earlier)?(2 points)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to apply maxpooling with stride = 1 (1 point)\n"
   ]
  },
  {
   "source": [
    "_Please type your answer to Task 03-2 here._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2>Convolutional Neural Network with Small Images</h2> \n",
    "\n",
    "In this part of the lab, we will use a Convolutional Neural Network to classify handwritten digits from the MNIST database. We will reshape the images with convolution to make them faster to process.\n",
    "\n",
    "Let's define the function <code>plot_channels</code> to plot out the kernel parameters of each channel. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    n_out = W.shape[0]\n",
    "    n_in = W.shape[1]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(n_out, n_in)\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "    out_index = 0\n",
    "    in_index = 0\n",
    "    \n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "        if in_index > n_in-1:\n",
    "            out_index = out_index + 1\n",
    "            in_index = 0\n",
    "        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index = in_index + 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "Define the function <code>plot_parameters</code> to plot out the kernel parameters of each channel with Multiple outputs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameters(W, number_rows=1, name=\"\", i=0):\n",
    "    W = W.data[:, i, :, :]\n",
    "    n_filters = W.shape[0]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_filters:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.suptitle(name, fontsize=10)    \n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "Define the function <code>plot_activation</code> to plot out the activations of the Convolutional layers  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A, number_rows=1, name=\"\", i=0):\n",
    "    A = A[0, :, :, :].detach().numpy()\n",
    "    n_activations = A.shape[0]\n",
    "    A_min = A.min().item()\n",
    "    A_max = A.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n",
    "    fig.subplots_adjust(hspace = 0.4)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_activations:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "Define the function <code>show_data</code> to plot out data samples as images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1]))"
   ]
  },
  {
   "source": [
    "<h3 id=\"Makeup_Data\">Get the Data</h3> \n",
    "\n",
    "Let's create a transform to resize the image and convert it to a tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "source": [
    "Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)"
   ]
  },
  {
   "source": [
    "Load the testing dataset by setting the parameters train  <code>False</code>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)"
   ]
  },
  {
   "source": [
    "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image that you have seen in Lab 02.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/imagenet.png\" width=\"550\" alt=\"MNIST data image\">\n",
    "\n",
    "Print out the fourth label. You will see y=1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The label for the fourth data element\n",
    "train_dataset[3][1]"
   ]
  },
  {
   "source": [
    "Plot the fourth sample using <code>show_data</code>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "show_data(train_dataset[3])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "<h3 id=\"CNN\">Build a Convolutional Neural Network Class</h3>\n",
    "\n",
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        \"\"\"Contructor\"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Prediction\"\"\"\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self, x):\n",
    "        \"\"\"outputs activation\"\"\"\n",
    "        z1 = self.cnn1(x)\n",
    "        a1 = torch.relu(z1)\n",
    "        out = self.maxpool1(a1)\n",
    "        \n",
    "        z2 = self.cnn2(out)\n",
    "        a2 = torch.relu(z2)\n",
    "        out1 = self.maxpool2(a2)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return z1, a1, z2, a2, out1,out"
   ]
  },
  {
   "source": [
    "<h3 id=\"Train\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model</h3> \n",
    "There are 16 output channels for the first layer, and 32 output channels for the second layer "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model object using CNN class\n",
    "model = CNN(out_1=16, out_2=32)"
   ]
  },
  {
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameters\n",
    "plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\n",
    "plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"
   ]
  },
  {
   "source": [
    "Define the loss function, the optimizer and the dataset loader."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "source": [
    "Train the model and determine validation/test accuracy. **(This may take a long time)**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_epochs=3\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "COST=0\n",
    "\n",
    "def train_model(n_epochs):\n",
    "    start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        COST=0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            COST+=loss.data\n",
    "        \n",
    "        cost_list.append(COST)\n",
    "        correct=0\n",
    "        #perform a prediction on the validation data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            z = model(x_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "    print('%.2f sec to train the basic CNN' % (time.time() - start))\n",
    "     \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "source": [
    "<h3 id=\"Result\">Analyze Results</h3> \n",
    "Plot the loss and accuracy on the validation data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list, color=color)\n",
    "ax1.set_xlabel('epoch', color=color)\n",
    "ax1.set_ylabel('Cost', color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color) \n",
    "ax2.set_xlabel('epoch', color=color)\n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "source": [
    "View the results of the parameters for the Convolutional layers "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the channels\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n",
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "source": [
    "Consider the following sample."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the second image\n",
    "show_data(train_dataset[1])"
   ]
  },
  {
   "source": [
    "Determine the activations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the CNN activations class to see the steps\n",
    "out = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "source": [
    "Plot out the first set of activation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Plot the outputs after the first CNN\n",
    "plot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "The image below is the result after applying the relu activation function "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outputs after the first Relu\n",
    "plot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")"
   ]
  },
  {
   "source": [
    "The image below is the result of the activation map after the second output layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Plot the outputs after the second CNN\n",
    "plot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "The image below is the result of the activation map after applying the second relu.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outputs after the second Relu\n",
    "plot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")"
   ]
  },
  {
   "source": [
    "## Task 04 - Analyze the result for the third training sample (10 points)\n",
    "\n",
    "Similar to the process outlined above, please write code to complete the following. \n",
    "* Display the third sample using <code>show_data</code>. (2 points)\n",
    "* Use the CNN activations class to see the steps in training for this sample. (4 points)\n",
    "* Plot the outputs after the first convolution. (1 point)\n",
    "* Plot the outputs after the first relu. (1 point)\n",
    "* Plot the outputs after the second convolution.(1 point)\n",
    "* Plot the outputs after the second relu. (1 point)\n",
    "\n",
    "Please type your code in the corresponding cells below. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to display the third sample using show_data. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to use the CNN activations class to see the steps in training for this sample. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to plot the outputs after the first convolution. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to plot the outputs after the first relu. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to plot the outputs after the second convolution.(1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to plot the outputs after the second relu. (1 point)"
   ]
  },
  {
   "source": [
    "## Task 05 - Analyze misclassified images in the MNIST validation dataset (10 points)\n",
    "\n",
    "Task 05-1: Earlier in the lab you saw how to plot the correctly classified and misclassified images. Complete the function <code>lab2_task5_1</code> that will print 5 misclassified digit images. (3 points)\n",
    "\n",
    "Task 05-2: __(This is a written answer question.)__ Please compare the level of misclassification of digits you saw earlier in this lab for Task 01 vs. the ones you see for this task. (2 points)\n",
    "\n",
    "Task 05-3: __(This is a written answer question.)__ What are your thoughts on the test accuracy of CNNs? (2 points)\n",
    "\n",
    "Task 05-4: __(This is a written answer question.)__ What are some strategies for increasing the accuracy? (3 points)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to complete this function that will print 5 misclassified digit images\n",
    "def lab2_task5_1():\n"
   ]
  },
  {
   "source": [
    "_(Please type your answer to Task 05-2 here.)_\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_(Please type your answer to Task 05-4 here.)_\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_(Please type your answer to Task 05-3 here.)_\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Part 3 - Applying CNN to Fashion-MNIST\n",
    "\n",
    "You may have noticed that even simple models achieve classification accuracy over 95% on MNIST, so it is hard to spot the differences between better models and weaker ones. In order to get a better intuition, we will use the qualitatively similar, but comparatively complex [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "Download the FashioMNIST data. Recall that the <code>ToTensor</code> Transform also moves the image channel from the last dimension to the first dimension to facilitate the convolutional neural network calculations introduced later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToTensor()\n",
    "fashion_mnist_train = dsets.FashionMNIST(root=\"./data\", train=True, transform=trans, target_transform=None, download=True)\n",
    "fashion_mnist_test = dsets.FashionMNIST(root=\"./data\", train=False, transform=trans, target_transform=None, download=True)"
   ]
  },
  {
   "source": [
    "The number of images for each category in the training set and the testing set is 6,000 and 1,000, respectively. Since there are 10 categories, the numbers of examples in the training set and the test set are 60,000 and 10,000, respectively."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fashion_mnist_train), len(fashion_mnist_test)"
   ]
  },
  {
   "source": [
    "We can access any example by indexing into the dataset using square brackets []. In the following code, we access the image and label corresponding to the first example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, label = fashion_mnist_train[0]"
   ]
  },
  {
   "source": [
    "Our example, stored here in the variable feature corresponds to an image with a height and width of 28 pixels. PyTorch automatically scales it into a tensor with each pixel value between 0 and 1. It is stored in a 3D Tensor. Its first dimension is the number of channels. Since the data set is a grayscale image, the number of channels is 1. When we encounter color images, we'll have 3 channels for red, green, and blue. To keep things simple, we will record the shape of the image with the height and width of $h$ and $w$ pixels, respectively, as $h \\times w$ or (h, w)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.shape, feature.dtype"
   ]
  },
  {
   "source": [
    "The label of each image is represented as a scalar in PyTorch. Its type is a 64-bit integer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, type(label)"
   ]
  },
  {
   "source": [
    "There are 10 categories in Fashion-MNIST: t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag and ankle boot. The following function can convert a numeric label into a corresponding text label.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "source": [
    "The following defines a function that can draw multiple images and corresponding labels in a single line."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fashion_mnist(images, labels):\n",
    "    # Here _ means that we ignore (not use) variables\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.reshape((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "source": [
    "Next, let's take a look at the image contents and text labels for the first nine examples in the training data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for idx, data in enumerate(fashion_mnist_train):\n",
    "    if(idx>=0 and idx<10):\n",
    "        X.append(data[0])\n",
    "        y.append(data[1])\n",
    "    if (idx>=10):\n",
    "        break\n",
    "# X, y = mnist_train[0:9]\n",
    "show_fashion_mnist(X, get_fashion_mnist_labels(y))"
   ]
  },
  {
   "source": [
    "To make our life easier when reading from the training and test sets we use a <code>DataLoader</code>. Recall that a <code>DataLoader</code> object reads a mini-batch of data with an example number of <code>batch_size</code> each time.\n",
    "\n",
    "In practice, reading data can often be a significant performance bottleneck for training, especially when the model is simple or when the computer is fast. A handy feature of PyTorch's <code>DataLoader</code> is the ability to use multiple processes to speed up data reading. For instance, we can set aside 4 processes to read the data (via <code>num_workers</code>)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "fashion_train_loader = DataLoader(fashion_mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
    "fashion_test_loader = DataLoader(fashion_mnist_test, batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "source": [
    "Let's apply a simple softmax regression to the fashionMNIST dataset (very similar to how we applied regression with mini-batch gradient descent to the MNIST digit dataset in lab 02). In softmax regression, we have as many outputs as there are categories. Because our dataset has $10$ categories, our network will have an output dimension of $10$. Consequently, our weights will constitute a $784 \\times 10$ matrix and the biases will constitute a $1 \\times 10$ vector. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = normal.Normal(loc = 0, scale = 0.01).sample((num_inputs, num_outputs))\n",
    "b = torch.zeros(num_outputs)"
   ]
  },
  {
   "source": [
    "Recall that we need to attach gradients to the model parameters. Here we are allocating memory for future gradients to be stored and notifiying PyTorch that we want gradients to be calculated with respect to these parameters in the first place."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.requires_grad_(True)\n",
    "b.requires_grad_(True)"
   ]
  },
  {
   "source": [
    "The softmax function is defined as follows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = torch.sum(X_exp, dim=1, keepdim=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "source": [
    "Now that we have defined the softmax operation, we can implement the softmax regression model. The below code defines the forward pass through the network. Note that we flatten each original image in the batch into a vector with length num_inputs with the view function before passing the data through our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "source": [
    "Loss function is defined as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "source": [
    "Given the predicted probability distribution <code>y_hat</code>, we typically choose the class with highest predicted probability whenever we must output a hard prediction. \n",
    "\n",
    "To compute accuracy we do the following: First, we execute <code>y_hat.argmax(dim=1)</code> to gather the predicted classes (given by the indices for the largest entries in each row). The result has the same shape as the variable y. Now we just need to check how frequently the two match. The result is PyTorch tensor containing entries of 0 (false) and 1 (true). Since the attribute mean can only calculate the mean of floating types, we also need to convert the result to float. Taking the mean yields the desired result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()"
   ]
  },
  {
   "source": [
    "Similarly, we can evaluate the accuracy for model net on the data set (accessed via <code>data_iter</code>)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).sum().item()\n",
    "        n += y.size()[0]  # y.size()[0] = batch_size\n",
    "    return acc_sum / n"
   ]
  },
  {
   "source": [
    "Train the model. **This may take some time**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_model_basic(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                # Use mini-batch gradient descent\n",
    "                for param in params:\n",
    "                    param.data.sub_(lr*param.grad/batch_size)\n",
    "                    param.grad.data.zero_()\n",
    "            else:\n",
    "                # This will be illustrated in the next section\n",
    "                trainer.step(batch_size)\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.size()[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train_model_basic(basic_net, fashion_train_loader, fashion_test_loader, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  },
  {
   "source": [
    "Now that training is complete, our model is ready to classify some images. Given a series of images, we will compare their actual labels (first line of text output) and the model predictions (second line of text output)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in fashion_test_loader:\n",
    "    break\n",
    "\n",
    "true_labels = get_fashion_mnist_labels(y.numpy())\n",
    "pred_labels = get_fashion_mnist_labels(basic_net(X).argmax(dim=1).numpy())\n",
    "titles = [truelabel + '\\n' + predlabel for truelabel, predlabel in zip(true_labels, pred_labels)]\n",
    "\n",
    "show_fashion_mnist(X[10:20], titles[10:20])"
   ]
  },
  {
   "source": [
    "### Applying CNN to Fashion-MNIST\n",
    "\n",
    "We are now ready to put all of the tools together to deploy your first fully-functional convolutional neural network for the Fashion-MNIST dataset. To make this data amenable to multilayer perceptrons which anticapte receiving inputs as one-dimensional fixed-length vectors, we first flatten each image, yielding vectors of length 784, before processing them with a series of fully-connected layers. The following two classes define the flattening and reshaping process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,1,28,28)"
   ]
  },
  {
   "source": [
    "The following neural net, implemented as a simple PyTorch [Sequential](https://pytorch.org/docs/master/generated/torch.nn.Sequential.html) module container, is one of the first published convolutional neural networks whose benefit was first demonstrated by Yann Lecun for the purpose of recognizing handwritten digits in images[LeNet5](http://yann.lecun.com/exdb/lenet/). (That is why we have named this network *le_net*)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_net = torch.nn.Sequential(\n",
    "    Reshape(),\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    nn.Linear(in_features=16*5*5, out_features=120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "source": [
    "Now that we've implemented the model, let's run some experiments to see what we can accomplish with the LeNet model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_updated(data_iter, net,device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    acc_sum,n = torch.tensor([0],dtype=torch.float32,device=device),0\n",
    "    for X,y in data_iter:\n",
    "        # If device is the GPU, copy the data to the GPU.\n",
    "        X,y = X.to(device),y.to(device)\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
    "            n += y.shape[0]\n",
    "    return acc_sum.item()/n"
   ]
  },
  {
   "source": [
    "Here is a simple function that we can use to detect whether we have a GPU. In it, we try to allocate gpu0 if available using torch.cuda.is_available() method. Otherwise, we stick with the CPU. For more information about PyTorch's device API, please see https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device.\n",
    "\n",
    "In your colab notebook settings, please remember to set the \"Runtime\" -> \"Change Runtime Type\" -> and in the \"Hardware accelerator\" drop-down list, select \"GPU.\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/gpu_settings.png\" width=\"200\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = try_gpu()"
   ]
  },
  {
   "source": [
    "Here is the training function. Note that we need to move each batch of data to our designated device (hopefully, the GPU) prior to making the forward and backward passes. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_lenet(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n",
    "        train_acc_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n",
    "        n, start = 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            net.train()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            X,y = X.to(device),y.to(device) \n",
    "            y_hat = net(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y = y.long()\n",
    "                train_l_sum += loss.float()\n",
    "                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
    "                n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy_updated(test_iter, net,device)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "source": [
    "The loss function and the training algorithm still use the cross-entropy loss function and mini-batch stochastic gradient descent. We did not cover Xavier initializer in class, and if you are interested in learning more please check the PyTorch documentation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.9, 5\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "le_net.apply(init_weights)\n",
    "le_net = le_net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "fashion_train_loader_le_net = DataLoader(fashion_mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
    "fashion_test_loader_le_net = DataLoader(fashion_mnist_test, batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "train_model_lenet(le_net, fashion_train_loader_le_net, fashion_test_loader_le_net, criterion,num_epochs, batch_size,device, lr)"
   ]
  },
  {
   "source": [
    "## Task 06 - Analyze classification accuracy of images in the Fashion-MNIST validation dataset (5 points)\n",
    "\n",
    "Write code to analyze the classification accuracy of the FashionMNIST validatiion dataset. You must compare the actual labels of the images against the model predictions. You may find it helpful to print the images as we have already seen for the output from the basic CNN. (5 points)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here to analyze the image classification results"
   ]
  },
  {
   "source": [
    "## Task 07 - Improve LeNet (10 points)\n",
    "\n",
    "Task 07-1: Construct a more complex network based on LeNet to improve its accuracy. You must demonstrate that your model has a higher accuracy compared to the vanilla <code>le_net</code> given in the lab. Be creative! (5 points)\n",
    "\n",
    "Here are several things you can try:\n",
    "* Adjust the convolution window size.\n",
    "* Adjust the number of output channels.\n",
    "* Adjust the activation function (e.g., try ReLU and/or Tanh).\n",
    "* Adjust the number of convolution layers.\n",
    "* Adjust the number of fully connected layers.\n",
    "* Adjust the learning rates and other training details (initialization, epochs, etc.)\n",
    "\n",
    "Task 07-2: Write code to try the new and improved <code>le_net</code> network on the FashionMNIST dataset. (2 points)\n",
    "\n",
    "Task 07-3: __(This is a written answer question.)__ Summarize the results of your trials in constructing the new and improved <code>le_net</code>. (3 points)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 07-1: Type your code here to improve le_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 07-2: Type your code here to apply the improved le_net on FashionMNIST"
   ]
  },
  {
   "source": [
    "_(Please type your answer to Task 07-3 here)_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task 08 - Other modern CNN Experimentation (3 points)\n",
    "\n",
    "Experiment with at least one other modern CNNs such as [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (or some other highly performant model for image data) on the Fashion MNIST dataset. Include your code in the designated cell below. Clearly indicate the model you utilized and the online documentation or the relevant academic paper containing more information about that model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 08 - Type your code here "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd07812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d",
   "display_name": "Python 3.9.0 64-bit ('3.9')"
  },
  "metadata": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}