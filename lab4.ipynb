{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python38064bit380pyenvc7bb6ca2a3f9479fafa53265e2886bf3",
      "display_name": "Python 3.8.0 64-bit ('3.8.0': pyenv)"
    },
    "colab": {
      "name": "lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqcCN4cRTb4L"
      },
      "source": [
        "# AI in Fact and Fiction - Summer 2021\n",
        "## Games and Deep Reinforcement Learning\n",
        "\n",
        "In this lab, we will explore reinforcement learning techniques and how they can be applied in games.\n",
        "\n",
        "* Use [Google Colab](https://colab.research.google.com/github/AIFictionFact/Summer2021/blob/master/lab4.ipynb) to run the python code, and to complete any missing lines of code.\n",
        "* You might find it helpful to save this notebook on your Google Drive.\n",
        "* For some of the tasks you may find it useful to have GPU enabled via 'Runtime $\\rightarrow$ Change runtime type' option.\n",
        "* Please make sure to fill the required information in the **Declaration** cell.\n",
        "* Once you complete the lab, please download the .ipynb file (File --> Download .ipynb).\n",
        "* Then, please use the following file naming convention to rename the downloaded python file lab4_YourRCS.ipynb (make sure to replace 'YourRCS' with your RCS ID, for example 'lab4_senevo.ipynb').\n",
        "* Submit the .ipynb file in LMS.\n",
        "\n",
        "<p>Due Date/Time: <b>Tuesday, Aug 10 1.00 PM ET</b></p>\n",
        "\n",
        "<p>Estimated Time Needed: <b>4 hours</b></p>\n",
        "\n",
        "<p>Total Tasks: <b>14</b></p>\n",
        "<p>Total Points: <b>50</b></p>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJplFC7iu8l_"
      },
      "source": [
        "\n",
        "**Declaration**\n",
        "\n",
        "*Your Name* :\n",
        "\n",
        "*Your RCS ID* :\n",
        "\n",
        "*Collaborators (if any)* :\n",
        "\n",
        "*Online Resources consulted (if any):*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9rhcUBcMi05"
      },
      "source": [
        "# Simple Tree Search Algorithms for Game Play\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlgioUEZMwdY"
      },
      "source": [
        "### Task 1 (4 points)\n",
        "\n",
        "In the following picture, suppose that you are playing 'X'. Assume that the reward for reaching a state only depends on that state, regardless of what will happen in the future.\n",
        "Given that, how many actions can you (as the 'X' player) take at this current board configuration, and what is the average reward of these actions? Please explain your answer. (This is a written answer question).\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AIFictionFact/Summer2021/main/images/tic-tac-toe.png\" alt=\"tictactoe\" width=\"200\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGm68P3RJzjV"
      },
      "source": [
        "_Please type your answer here. (4 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1VjG2a9QCI8"
      },
      "source": [
        "## Simple Program to Solve Tic-Tac-Toe Board Configurations\n",
        "\n",
        "In the class we discussed algorithms such as [MiniMax](https://en.wikipedia.org/wiki/Minimax) for game play.\n",
        "\n",
        "The Minimax Algorithm is a decision rule formulated for two-player zero-sum games (Tic-Tac-Toe, Chess, Go, etc.). This algorithm sees a few steps ahead and puts itself in the shoes of its opponent. It keeps playing and exploring possible subsequent states until it reaches a terminal state resulting in a draw, a win, or a loss. \n",
        "\n",
        "We will first explore the application of basic tree search algorithms such as MiniMax for the Tic-Tac-Toe board game.\n",
        "\n",
        "In our execution of the Minimax algorithm for solving Tic-Tac-Toe, it works by visualizing all possible future states of the board and constructs them in the form of a tree. When the current board state is given to the algorithm (the root of the tree), it splits into `n` branches (where n denotes the number of moves chosen by the AI depending on the number of empty cells where the AI can be placed). If any of these new states is a terminal state, no further splits are performed for this state, and we get a winner!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQO7VNItQIDm"
      },
      "source": [
        "import copy\n",
        "\n",
        "class TicTacToeMiniMaxSolver:\n",
        "  '''Simple tree search algorithm to solve any tic tac toe position'''\n",
        "\n",
        "  # Square definitions\n",
        "  X_SQUARE = 'X'\n",
        "  O_SQUARE = 'O'\n",
        "  BLANK = '_'\n",
        "\n",
        "  # Evaluation definitions\n",
        "  X_WINS = 'X wins!'\n",
        "  O_WINS = 'O wins!'\n",
        "  DRAW = 'Draw!'\n",
        "\n",
        "  def is_X_turn(pos):\n",
        "    '''Returns true if X's turn to move, false otherwise'''\n",
        "    x_count = 0\n",
        "    for row in pos:\n",
        "      x_count += row.count(X_SQUARE)\n",
        "      x_count -= row.count(O_SQUARE)\n",
        "    return x_count == 0\n",
        "  \n",
        "  def is_full(pos):\n",
        "    '''Returns true if every space is taken, false otherwise'''\n",
        "    for row in pos:\n",
        "      if BLANK in row:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  def get_branches(pos, X_turn):\n",
        "    '''Takes a position, and returns a list of every position that can result from a move'''\n",
        "    symbol = X_SQUARE if X_turn else O_SQUARE\n",
        "    branches = []\n",
        "    for row in range(3):\n",
        "      for square in range(3):\n",
        "        if pos[row][square] == BLANK:\n",
        "          branches.append(copy.deepcopy(pos))\n",
        "          branches[-1][row][square] = symbol\n",
        "    return branches\n",
        "\n",
        "  def get_static_eval(pos):\n",
        "    '''Checks for three in a row in the current position, returns evaluation'''\n",
        "    potential_wins = []\n",
        "\n",
        "    # Three in a row\n",
        "    for row in pos:\n",
        "      potential_wins.append(set(row))\n",
        "\n",
        "    # Three in a column\n",
        "    for i in range(3):\n",
        "      potential_wins.append(set([pos[k][i] for k in range(3)]))\n",
        "\n",
        "    # Three in a diagonal\n",
        "    potential_wins.append(set([pos[i][i] for i in range(3)]))\n",
        "    potential_wins.append(set([pos[i][2 - i] for i in range(3)]))\n",
        "\n",
        "    # Checking if any three are the same\n",
        "    for trio in potential_wins:\n",
        "      if trio == set([X_SQUARE]):\n",
        "        return X_WINS\n",
        "      elif trio == set([O_SQUARE]):\n",
        "        return O_WINS\n",
        "    return DRAW\n",
        "\n",
        "  def solve(pos):\n",
        "    '''Returns the dynamic evaluation of any valid position'''\n",
        "    \n",
        "    # Immediately return the static evaluation if it is decisive\n",
        "    static_eval = get_static_eval(pos)\n",
        "    if static_eval != DRAW:\n",
        "      return static_eval\n",
        "\n",
        "    # Check for full board\n",
        "    if is_full(pos):\n",
        "      return DRAW\n",
        "\n",
        "    # Checking and evaluating every path\n",
        "    X_turn = is_X_turn(pos)\n",
        "    branches = get_branches(pos, X_turn)\n",
        "    branch_evals = [solve(branch) for branch in branches]\n",
        "\n",
        "    # Returning the result assuming best play\n",
        "    if X_turn:\n",
        "      # X options from best to worst\n",
        "      if X_WINS in branch_evals:\n",
        "        return X_WINS\n",
        "      elif DRAW in branch_evals:\n",
        "        return DRAW\n",
        "      else:\n",
        "        return O_WINS\n",
        "    else:\n",
        "      # O options from best to worst\n",
        "      if O_WINS in branch_evals:\n",
        "        return O_WINS\n",
        "      elif DRAW in branch_evals:\n",
        "        return DRAW\n",
        "      else:\n",
        "        return X_WINS"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xcP0_06RBS7"
      },
      "source": [
        "We define board positions in the following way. For example, this is one of the moves X can make. We can feed this into our simple program, and see what the outcome might be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPpyY6HxS_MY",
        "outputId": "c74cbd17-eaa1-4a55-fe78-84283ef2fbda"
      },
      "source": [
        "x_move_1 =     [['X', '_', '_'],\n",
        "               ['_', '_', '_'],\n",
        "               ['_', '_', '_']]\n",
        "print(TicTacToeMiniMaxSolver.solve(x_move_1)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X wins!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsgbXpHdXVob"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zKbQehbUGcB"
      },
      "source": [
        "### Task 2 (2 points)\n",
        "\n",
        "Define the TicTacToe board position given in Task 1, and obtain the outcome from the `TicTacToeSolver.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1jbqsuHUfOM"
      },
      "source": [
        "# Type your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0SOgIJzojm2"
      },
      "source": [
        "### Task 3 (4 points)\n",
        "\n",
        "Come up with a board configuration with an initial `X` move and an `O` move that guarantees a definite win for `X`. It is okay if your solution is a brute force algorithm. \n",
        "\n",
        "For example, the following configuration is a definite win for `X`.\n",
        "\n",
        "x_wins =    [['X', 'O', '\\_'],\n",
        "            ['\\_', '\\_', '\\_'],\n",
        "            ['\\_', '\\_', '\\_']]\n",
        "\n",
        "Your code should output such a configuration (excluding the one above) with 2 moves (`X` move followed by `O` move) that results in a definite win for `X`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFMVbU0arzST"
      },
      "source": [
        "# Type your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JVYOZnor7AD"
      },
      "source": [
        "### Task 4 (2 points)\n",
        "\n",
        "Inspect the `TicTacToeSolver` carefully. What are some optimizations that can be made to the code? Please provide at least 2 optimizations. _(This is a written answer question)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv8auFxDsUZl"
      },
      "source": [
        "_(Type your answer here)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6d9abEDYR4"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "The field of deep learning is inspired by natural intelligence, and reinforcement learning is no exception. Consider a baby learning to walk, a bird learning to fly, or an RL agent trying to land a spaceship. They all have these three things common:\n",
        "\n",
        "1. Trial and Error: Each agent (baby, bird, or RL agent) makes many unsuccessful attempts--learning from each failure.\n",
        "2. Goal: The agent has a specific goal (to stand, fly, or land the spaceship).\n",
        "3. Interaction with the environment: There is no manual, no teacher, no training sample from which it can learn. The only feedback is the feedback from the immediate environment. The feedback is in terms of some reward or punishment.\n",
        "\n",
        "In reinforcement learning, an agent takes a sequence of actions in an uncertain and often complex environment with the goal of maximizing a reward function. Essentially, it is an approach for making appropriate decisions in a game-like environment that maximizes rewards and minimizes penalties. Feedback from its own actions and experience allows the agent to learn the most appropriate action by trial and error. Generally, reinforcement learning involves the following steps:\n",
        "\n",
        "1. Observing the environment\n",
        "2. Formulating a decision based on a certain strategy\n",
        "3. Actions\n",
        "4. Receiving a reward or penalty\n",
        "5. Learning from the experiences to improve the strategy\n",
        "6. Iteration of the process until an optimal strategy is achieved\n",
        "\n",
        "There’s quite a lot that you can do with reinforcement learning – whether it’s related to video games or not. The core skills can be used across a variety of purposes, from stock trading and finance to cybersecurity and art.\n",
        "\n",
        "We will first apply reinforcement learning to teach and AI to play Tic-Tac-Toe with a human. In this game play, as you saw earlier, an agent takes actions within an environment. Based on these actions, the agent achieves different states with different rewards. For example, in Tic-Tac-Toe, your reward might be 1 if you got three-in-a-row, −1 if your opponent got three-in-a-row, and 0 otherwise. Your state space would consist of all possible board configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSzQR4PTv24j"
      },
      "source": [
        "## Exploitation vs. Exploration\n",
        "\n",
        "One of the fundamental tradeoffs in reinforcement learning is the exploitation vs. exploration tradeoff. \n",
        "\n",
        "**Exploitation** means choosing the action which maximizes our reward (which may lead to being stuck in a local optimum). \n",
        "\n",
        "**Exploration** means choosing an action regardless of the reward it provides (this helps us discover other local optimum solutions that may lead us closer to the global optimum). \n",
        "\n",
        "Going all out in either one of them is harmful; all exploitation may lead to a suboptimal agent, and all exploration would give us a not-so-intelligent agent which keeps taking random actions.\n",
        "\n",
        "A widely used strategy to tackle this problem, is the **epsilon-decreasing strategy**. It works as follows:\n",
        "1. Initialize a variable `epsilon` with a value between 0 and 1.\n",
        "2. Now with probability = `epsilon`, we explore, and with probability = `1-epsilon`, we exploit.\n",
        "3. We decrease the value of `epsilon` over time until it becomes zero.\n",
        "\n",
        "Using this strategy, the agent can explore better actions during the earlier stages of the training, and then it exploits the best actions in the later stages of the game.\n",
        "\n",
        "Say, if a state leads to the AI winning, it shall have a positive value (`value = 1`). If AI loses in some state, it shall have a negative value (`value = -1`). All the rest of the states would have a neutral value (`value = 0`). These are the initialized state values.\n",
        "\n",
        "Once a game has started, our agent computes all possible actions it can take in the current state and the new states which would result from each action. The values of these states are collected from a `state_value` vector, which contains values for all possible states in the game. The agent can then choose the action that leads to the state with the highest value (exploitation) or chooses a random action (exploration), depending on the epsilon value. Throughout our training, we play several games. After each move, the value of the state is updated using the following rule:\n",
        "\n",
        "$$ V(s) \\leftarrow V(s) + \\alpha \\times (V(s^f) - V(s))$$\n",
        "\n",
        "where,\n",
        "\n",
        "* $V(s)$ = current state of the game board\n",
        "* $V(s^f)$ = the new state of the board after agent takes some action\n",
        "* $\\alpha$ = learning rate (or the step-size parameter)\n",
        "\n",
        "Using this update rule, the states that lead to a loss, get a negative state value (whose magnitude depends on the learning rate). The agent learns that being in such a state may lead to a loss down the line, so it would try to avoid landing in this state unless necessary. On the other hand, the states that lead to a win, get a positive state value. The agent learns that being in such a state may lead to a win down the line, so it would be encouraged to be in this state.\n",
        "\n",
        "An implementation for this algorithm for the Tic-Tac-Toe game play is available in the [this repository](https://github.com/AIFictionFact/tic-tac-toe-bot). Please see [HumanVsAI_RLTest.py](https://github.com/AIFictionFact/tic-tac-toe-bot/blob/master/HumanVsAI_RLTest.py). \n",
        "\n",
        "Let's see how the AI plays TicTacToe. \n",
        "\n",
        "First clone the repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEoeqg0LxzNH",
        "outputId": "d66447c7-91b5-45c6-e764-f47bee58fdbf"
      },
      "source": [
        "!rm -rf tic-tac-toe-bot\n",
        "!git clone https://github.com/AIFictionFact/tic-tac-toe-bot.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tic-tac-toe-bot'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 76 (delta 15), reused 16 (delta 7), pack-reused 49\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clNyoaQQ17L"
      },
      "source": [
        "Then, let's run the `HumanVsAI_RLTest.py` file. \n",
        "\n",
        "Please note that the \"board position number\" corresponds to the following positions.\n",
        "<code> \n",
        " ---------------\n",
        "  | 1 || 2 || 3 |\n",
        "  ---------------\n",
        "  | 4 || 5 || 6 |\n",
        "  ---------------\n",
        "  | 7 || 8 || 9 |\n",
        "  ---------------\n",
        "</code>\n",
        "\n",
        "\n",
        "Use the `HumanVsAI_RLTest.py` to play a game with an RL agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ta2annx2Qr"
      },
      "source": [
        "!python tic-tac-toe-bot/HumanVsAI_RLTest.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh1QZfOcffxI"
      },
      "source": [
        "### Task 5 (4 points)\n",
        "\n",
        "Use the `HumanVsAI_RLTest.py` to play several games with an RL agent. Can you win the game with the RL agent? If so, provide the winning board configuration and the output in your answer. (Please note that a \"win\" does not include \"draw\").\n",
        "If you cannot win against the RL agent, please explain why you may not be able to. (2 points)\n",
        "\n",
        "If we had the MiniMax algorithm implemented as an agent, would you be able to win against the MiniMax agent? Please provide reasons. (2 points)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb1GgZCWhWKb"
      },
      "source": [
        "_(Please type your answer here)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD-Zgby4mvAl"
      },
      "source": [
        "## Training Reinforcement Learning Algorithms\n",
        "\n",
        "The above RL agent was trained by letting two AI agents play with each other for 10,000 epochs. When the agents are training, they use the exploit-explore method discussed earlier. The following code shows how the gameplay happens when both the players are AI, where each of them helps train each other. The number of epochs has been shortened to 100. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUovfRFciC_f"
      },
      "source": [
        "!python tic-tac-toe-bot/AIVsAI_RL_Train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMtWGYvzm3my"
      },
      "source": [
        "### Task 6 (3 points)\n",
        "\n",
        "How does the agent decide when to explore and exploit? You may check the source code of [AIVsAI_RL_Train.py](https://github.com/AIFictionFact/tic-tac-toe-bot/blob/master/AIVsAI_RL_Train.py) for your answer. _(This is a written answer question)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dxiSSPEoTqI"
      },
      "source": [
        "_Please type your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJXKvHPHz-TA"
      },
      "source": [
        "# Introduction to the OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments. The goal is to standardize how environments are defined in AI research publications so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. \n",
        "\n",
        "Because OpenAI Gym requires a graphics display, the only (easy) way to display Gym in Google CoLab is an embedded video.  The presentation of OpenAI Gym game animations in Google CoLab is discussed later, and we have presented two approaches for generating the videos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDVfHkxLA6G1"
      },
      "source": [
        "Let's install the required libraries. First is the OpenAI Gym."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NKc4iUc645J"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epTWUtW3BEst"
      },
      "source": [
        "Since Colab doesn’t have a display except the Notebook in HTML, when we train reinforcement learning model with OpenAI Gym, we encounter `NoSuchDisplayException` when calling `gym.Env.render()` method. Therefore we need to install additional software to make it work.\n",
        "\n",
        "First, we require the virtual X11 display [Xvfb](https://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HKFYUqqBllO"
      },
      "source": [
        "!apt update\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5gj--y6BqvH"
      },
      "source": [
        "Additionaly, to launch X virtual frame buffer (Xvfb) from Notebook, install [PyVirtualDisplay](https://github.com/ponty/PyVirtualDisplay)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPedgfThByKB"
      },
      "source": [
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTjzpVh4B1Hn"
      },
      "source": [
        "Initialize the display."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OME3FsdaB7r1"
      },
      "source": [
        "import pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYccyyMfB8--"
      },
      "source": [
        "Let's use the official `gym.wrappers.Monitor` and store the display animation as a movie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RigAFuhvCOGP"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def wrap_env(env):\n",
        "  \"\"\"\n",
        "  Utility functions to enable video recording of gym environment \n",
        "  and displaying it.\n",
        "  To enable video, just do \"env = wrap_env(env)\"\"\n",
        "  \"\"\"\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb94jHGy6W1n"
      },
      "source": [
        "### Looking at Gym Environments\n",
        "\n",
        "The centerpiece of Gym is the environment, which defines the \"game\" in which your reinforcement algorithm will compete.  An environment does not need to be a game; however, it describes the following game-like features:\n",
        "* **Action space**: What actions can we take on the environment, at each step/episode, to alter the environment.\n",
        "* **Observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
        "\n",
        "Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.\n",
        "\n",
        "* **Agent** - The machine learning program or model that controls the actions.\n",
        "* **Step** - One round of issuing actions that affect the observation space.\n",
        "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective, or the episode reaches the maximum number of allowed steps.\n",
        "* **Render** - Gym can render one frame for display after each episode.\n",
        "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
        "* **Nondeterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
        "\n",
        "It is important to note that many of the gym environments specify that they are not nondeterministic even though they make use of random numbers to process actions. It is generally agreed upon (based on the gym GitHub issue tracker) that nondeterministic property means that a deterministic environment will still behave randomly even when given consistent seed value. The seed method of an environment can be used by the program to seed the random number generator for the environment.\n",
        "\n",
        "The Gym library allows us to query some of these attributes from environments.  The following function can be used to query gym environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CNllTTe6h3P"
      },
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4QGAnPS6oN8"
      },
      "source": [
        "### Classic Control Environments\n",
        "\n",
        "We will begin by looking at the \"CartPole-v0\" environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls5WlnEO8bk1",
        "outputId": "ff5fb152-ad3f-486d-e3f9-f17a7d5fafd0"
      },
      "source": [
        "query_environment(\"CartPole-v0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 195.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqXImKGf8w18"
      },
      "source": [
        "The CartPole-v0 environment challenges the agent to move a cart while keeping a pole balanced. The environment has an observation space of 4 continuous numbers:\n",
        "\n",
        "* Cart Position\n",
        "* Cart Velocity\n",
        "* Pole Angle\n",
        "* Pole Velocity At Tip\n",
        "\n",
        "To achieve this goal, the agent can take the following actions:\n",
        "\n",
        "* Push cart to the left\n",
        "* Push cart to the right\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP3qj-jRImmp"
      },
      "source": [
        "Let's call the CartPole environment and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg6HW3fAIrr8"
      },
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpPuz27v-O6V"
      },
      "source": [
        "### Task 7 (2 points)\n",
        "\n",
        "Let's consider another gym environment called the \"MountainCar-v0\", which challenges an underpowered car to escape the valley between two mountains. Write the code to query the environment (1 point) and describe the Mountain Car environment (1 point).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYBMnRFv_HZa"
      },
      "source": [
        "# Type your code here to display the environment (1 point)\n",
        "\n",
        "\n",
        "# Type your code here to query the environment (1 point)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VxxokOmEh8d"
      },
      "source": [
        "### Task 8 (6 points) \n",
        "\n",
        "What can you say about the actions of the agent (2 points), the observations (2 points), and the reward (2 points) in the MountainCar environment? _(This is a written answer question.)_\n",
        "\n",
        "Hint: It may be helpful to take a look at the online documentation and the source code of the [Mountain-Car environment](https://gym.openai.com/envs/MountainCar-v0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1WpSIDcF5pI"
      },
      "source": [
        "_Type your answer here._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OdUzcP_F-sT"
      },
      "source": [
        "### Atari Games\n",
        "\n",
        "Atari games can use an observation space that is either equal to the size of the Atari screen (210x160) or even use the RAM memory of the Atari (128 bytes) to determine the state of the game. Yes thats bytes, not kilobytes!\n",
        "\n",
        "We will first need to load the Atari ROMs into our Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6mi6R9iHNq8"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow-wzHDiHko_"
      },
      "source": [
        "Let's query a sample game, for example Breakout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J-TpAPBGXNt"
      },
      "source": [
        "query_environment(\"Breakout-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3FGYl7GHzSd"
      },
      "source": [
        "Let's visualize the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBxHJQR8IJbt"
      },
      "source": [
        "env = wrap_env(gym.make(\"Breakout-v0\"))\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqHTCGJyqoSl"
      },
      "source": [
        "### Task 9 (5 points)\n",
        "\n",
        "Select a game of your choice from the list of Atari ROMs downloaded (except \"Breakout_v0\"), query it's environment, and render it in a video.  _(Please provide the code below)_\n",
        "\n",
        "Interpret the action space, the observation space, and the reward based on the parameters available in the environment as well as the video output. _(This is a written answer question)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc8hSH0sq2_4"
      },
      "source": [
        "# Type your code here to query the environment (1 point)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFfOVfYYq3cL"
      },
      "source": [
        "# Type your code here to display the environment (1 point)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4sEZTNNtkEL"
      },
      "source": [
        "_Type your written answer here to interpret the environment (3 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvYBNxCFhyJD"
      },
      "source": [
        "# Introduction to Q-Learning\n",
        "\n",
        "Q-Learning is a foundational technique upon which deep reinforcement learning is based.  Before we explore deep reinforcement learning, it is essential to understand Q-Learning.  \n",
        "\n",
        "Several components make up any Q-Learning system, and you have encuntered many of these components before.\n",
        "\n",
        "* **Agent** - The agent is an entity that exists in an environment that takes actions to affect the state of the environment, to receive rewards.\n",
        "* **Environment** - The environment is the universe that the agent exists in.  The environment is always in a specific state that is changed by the actions of the agent.\n",
        "* **Actions** - Steps that can be performed by the agent to alter the environment \n",
        "* **Step** - A step occurs each time that the agent performs an action and potentially changes the environment state.\n",
        "* **Episode** - A chain of steps that ultimately culminates in the environment entering a terminal state.\n",
        "* **Epoch** - A training iteration of the agent that contains some number of episodes.\n",
        "* **Terminal State** -  A state in which further actions do not make sense.  In many environments, a terminal state occurs when the agent has won, lost, or the environment exceeding the maximum number of steps.\n",
        "\n",
        "Q-Learning works by building a table that suggests an action for every possible state.  Q-Learning primarily deals with discrete actions, such as pressing a joystick up or down. Out of the box, Q-Learning does not deal with continuous inputs, such as a car's accelerator that can be in a range of positions from released to fully engaged. However, researchers have come up with clever tricks to allow Q-Learning to accommodate continuous actions. Q-Learning handles continuous states by binning these numeric values into ranges. Furthermore, deep neural networks can help to solve the problems of continuous environments and action spaces.  \n",
        "The agent must bin continuous state values into a fixed finite number of columns.\n",
        "\n",
        "Learning occurs when the algorithm runs the agent and environment through a series of episodes and updates the Q-values based on the rewards received from actions taken.\n",
        "\n",
        "The Q-values can dictate action by selecting the action column with the highest Q-value for the current environment state.  The choice between choosing a random action and a Q-value driven action is governed by the epsilon ($\\epsilon$) parameter, which is the probability of random action.\n",
        "\n",
        "Each time through the training loop, the training algorithm updates the Q-values according to the following equation.\n",
        "\n",
        "$Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{\\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}}}_{\\text{new value (temporal difference target)}} - \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} \\bigg) }^{\\text{temporal difference}}$\n",
        "\n",
        "There are several parameters in this equation:\n",
        "* alpha ($\\alpha$) - The learning rate, how much should the current step cause the Q-values to be updated.\n",
        "* lambda ($\\lambda$) - The discount factor is the percentage of future reward that the algorithm should consider in this update.\n",
        "\n",
        "This equation modifies several values:\n",
        "\n",
        "* $Q(s_t,a_t)$ - The Q-table.  For each combination of states, what reward would the agent likely receive for performing each action?\n",
        "* $s_t$ - The current state.\n",
        "* $r_t$ - The last reward received.\n",
        "* $a_t$ - The action that the agent will perform.\n",
        "\n",
        "The equation works by calculating a delta (temporal difference) that the equation should apply to the old state.  This learning rate ($\\alpha$) scales this delta.  A learning rate of 1.0 would fully implement the temporal difference to the Q-values each iteration and would likely be very chaotic.\n",
        "\n",
        "There are two parts to the temporal difference: the new and old values.  The new value is subtracted from the old value to provide a delta; the full amount that we would change the Q-value by if the learning rate did not scale this value.  The new value is a summation of the reward received from the last action and the maximum of the Q-values from the resulting state when the client takes this action. It is essential to add the maximum of action Q-values for the new state because it estimates the optimal future values from proceeding with this action. \n",
        "\n",
        "For now, we will apply regular Q-Learning to the Mountain Car problem from OpenAI Gym.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_ME3GyjrIsH"
      },
      "source": [
        "**Simple Algorithm for the Mountain Car**\n",
        "\n",
        "The following code shows an agent that applies full throttle to climb the hill. The cart is not strong enough. It will need to use potential energy from the mountain behind it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eBGlVDfrT6y"
      },
      "source": [
        "mountain_car_simple_env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "\n",
        "mountain_car_simple_env.reset()\n",
        "\n",
        "done = False\n",
        "\n",
        "i = 0\n",
        "while not done:\n",
        "    i += 1\n",
        "    state, reward, done, _ = mountain_car_simple_env.step(2)\n",
        "    mountain_car_simple_env.render()\n",
        "    print(f\"Step {i}: State={state}, Reward={reward}\")\n",
        "    \n",
        "mountain_car_simple_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQRqQbszMQt"
      },
      "source": [
        "Let's also see the mountain car in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri1FNR74r5R3"
      },
      "source": [
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U1W8YWJsHt6"
      },
      "source": [
        "### Task 10 (3 points)\n",
        "\n",
        "Similar to the above program, write code to program the mountain car such that it always applies force to one direction or another. Whatever direction the vehicle is currently rolling, the agent uses power in that direction. Therefore, if the car begins to climb a hill, it would be overpowered by gravity, and turns backward. However, once it starts to roll backward force is immediately applied in this new direction to gather enough potential energy to climb the hill. (2 points)\n",
        "\n",
        "Visualize the preprogrammed car with the above solution. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlCCY6yds4U5"
      },
      "source": [
        "# Type your code to program the mountain car (2 points)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtBs3zss9us"
      },
      "source": [
        "# Type your code to visualize the mountain car environment (1 point)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZACe0PpuPun"
      },
      "source": [
        "**Q-Learning Car**\n",
        "\n",
        "We will now use Q-Learning to produce a car that learns to drive itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KtHD2couXzF"
      },
      "source": [
        "def calc_discrete_state(env, state):\n",
        "    '''\n",
        "    This function converts the floating point state values into \n",
        "    discrete values. This is often called binning.  We divide \n",
        "    the range that the state values might occupy and assign \n",
        "    each region to a bucket.\n",
        "    '''\n",
        "    discrete_state = (state - env.observation_space.low)/buckets\n",
        "    return tuple(discrete_state.astype(np.int))  \n",
        "\n",
        "def run_game(env, q_table, render, should_update):\n",
        "      '''\n",
        "      Run one game.  The q_table to use is provided.  We also \n",
        "      provide a flag to indicate if the game should be \n",
        "      rendered/animated.  Finally, we also provide\n",
        "      a flag to indicate if the q_table should be updated.\n",
        "      '''\n",
        "      done = False\n",
        "      discrete_state = calc_discrete_state(env, env.reset())\n",
        "      success = False\n",
        "        \n",
        "      while not done:\n",
        "          # Exploit or explore\n",
        "          if np.random.random() > epsilon:\n",
        "              # Exploit - use q-table to take current best action \n",
        "              # (and probably refine)\n",
        "              action = np.argmax(q_table[discrete_state])\n",
        "          else:\n",
        "              # Explore - t\n",
        "              action = np.random.randint(0, env.action_space.n)\n",
        "              \n",
        "          # Run simulation step\n",
        "          new_state, reward, done, _ = env.step(action)\n",
        "          \n",
        "          # Convert continuous state to discrete\n",
        "          new_state_disc = calc_discrete_state(env, new_state)\n",
        "\n",
        "          # Have we reached the goal position (have we won?)?\n",
        "          if new_state[0] >= env.unwrapped.goal_position:\n",
        "              success = True\n",
        "            \n",
        "          # Update q-table\n",
        "          if should_update:\n",
        "              max_future_q = np.max(q_table[new_state_disc])\n",
        "              current_q = q_table[discrete_state + (action,)]\n",
        "              new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * \\\n",
        "                (reward + DISCOUNT * max_future_q)\n",
        "              q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "          discrete_state = new_state_disc\n",
        "          \n",
        "          if render:\n",
        "              env.render()\n",
        "              \n",
        "      return success\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_WxkIRGvOwd"
      },
      "source": [
        "### Hyperparameters in Q-Learning\n",
        "\n",
        "Several hyperparameters are very important for Q-Learning. These parameters will likely need adjustment as you apply Q-Learning to other problems.  Because of this, it is crucial to understand the role of each parameter.\n",
        "\n",
        "* **LEARNING_RATE** The rate at which previous Q-values are updated based on new episodes run during training. \n",
        "* **DISCOUNT** The amount of significance to give estimates of future rewards when added to the reward for the current action taken.  A value of 0.95 would indicate a discount of 5% to the future reward estimates. \n",
        "* **EPISODES** The number of episodes to train over.  Increase this for more complex problems; however, training time also increases.\n",
        "* **SHOW_EVERY** How many episodes to allow to elapse before showing an update.\n",
        "* **DISCRETE_GRID_SIZE** How many buckets to use when converting each of the continuous state variables.  For example, [10, 10] indicates that the algorithm should use ten buckets for the first and second state variables.\n",
        "* **START_EPSILON_DECAYING** Epsilon is the probability that the agent will select a random action over what the Q-Table suggests. This value determines the starting probability of randomness.\n",
        "* **END_EPSILON_DECAYING** How many episodes should elapse before epsilon goes to zero and no random actions are permitted. For example, EPISODES//10  means only the first 1/10th of the episodes might have random actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uLgB5yjvYBr"
      },
      "source": [
        "# Q-Learning hyper parameters\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 50000\n",
        "SHOW_EVERY = 1000\n",
        "\n",
        "DISCRETE_GRID_SIZE = [10, 10]\n",
        "START_EPSILON_DECAYING = 0.5\n",
        "END_EPSILON_DECAYING = EPISODES//10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKhFTLzvfMm"
      },
      "source": [
        "We can now make the environment. \n",
        "\n",
        "Warning: this code may take some time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8QFmZOYvf7q"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "mountain_car_qlearning_env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "\n",
        "epsilon = 1  \n",
        "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "buckets = (mountain_car_qlearning_env.observation_space.high - mountain_car_qlearning_env.observation_space.low) / DISCRETE_GRID_SIZE\n",
        "q_table = np.random.uniform(low=-3, high=0, size=(DISCRETE_GRID_SIZE + [mountain_car_qlearning_env.action_space.n]))\n",
        "success = False\n",
        "\n",
        "episode = 0\n",
        "success_count = 0\n",
        "\n",
        "# Loop through the required number of episodes\n",
        "while episode<EPISODES:\n",
        "    episode+=1\n",
        "    done = False\n",
        "\n",
        "    # Run the game.  If we are local, display render animation at SHOW_EVERY\n",
        "    # intervals. \n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        print(\"Current episode:\", episode, \"success:\", success_count , float(success_count)/SHOW_EVERY)\n",
        "        success = run_game(mountain_car_qlearning_env, q_table, True, False)\n",
        "        success_count = 0\n",
        "    else:\n",
        "        success = run_game(mountain_car_qlearning_env, q_table, False, True)\n",
        "        \n",
        "    # Count successes\n",
        "    if success:\n",
        "        success_count += 1\n",
        "\n",
        "    # Move epsilon towards its ending value, if it still needs to move\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon = max(0, epsilon - epsilon_change)\n",
        "\n",
        "print(success)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSIkLVST_XZH"
      },
      "source": [
        "**Running and Observing the Agent**\n",
        "\n",
        "Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following code to see the agent in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qme-fXCK_6gB"
      },
      "source": [
        "run_game(mountain_car_qlearning_env, q_table, True, False)\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjYWsKi-v-58"
      },
      "source": [
        "### Task 11 (2 points)\n",
        "\n",
        "Observe the success rate that gets output for the Q-learning algorithm above. Notice that the number of successful episodes generally increases as training progresses. However, there are some earlier episodes in which a success rate of 1.0 was achieved. Could we have stoped the learning process at that point? Explain your answer. _(This is a written answer question)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MBbL5LxwsLo"
      },
      "source": [
        "_(Please type your answer here)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXnTAuOav6xP"
      },
      "source": [
        "# Introduction to a Deep Reinforcement Library\n",
        "\n",
        "In this part of the lab, we will explore one of the cutting-edge deep-reinforcement libraries called [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3). This library contains a set of reliable implementations of reinforcement learning algorithms in PyTorch.\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/).\n",
        "\n",
        "**Important:**\n",
        "You might want to change the Hardware accelerator in the Runtime menu to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87WRY7w4wuZa"
      },
      "source": [
        "Let's first install the Stable Baselines3 package. The `[extra]` part includes optional dependencies like Tensorboard, OpenCV or atari-py to train on atari games."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QtpClaKwxcj"
      },
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AVS9kb70uFt"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option.\n",
        "\n",
        "We chose the [MlpPolicy](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html?highlight=MlpPolicy#stable_baselines3.dqn.MlpPolicy), which implements an [actor critic algorithm](https://papers.nips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf). \n",
        "\n",
        "**Note:** You are not expected to know all the details of the policies and algorithms available for the purpose of this lab. But please make sure you know how to apply them when developing an agent for a particular environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFQhWJm1oYg"
      },
      "source": [
        "from stable_baselines3.ppo import MlpPolicy, PPO"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbeJPaV03naK"
      },
      "source": [
        "## Deep-RL with Control Problems\n",
        "\n",
        "For this example, we will use the CartPole environment, which you saw earlier.\n",
        "\n",
        "Stable-baselines provides a set of default [policies](https://stable-baselines.readthedocs.io/en/master/modules/policies.html), that can be used with most action spaces. \n",
        "\n",
        "We chose the [MlpPolicy](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html?highlight=MlpPolicy#stable_baselines3.dqn.MlpPolicy) because input of CartPole is a feature vector, not images (like in the case of Atari games).\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space.\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization (PPO)](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQCM9G_T3Rp8",
        "outputId": "63f00c0a-c541-4e19-92d8-50d49acdaf01"
      },
      "source": [
        "cartpole_env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "cartpole_env._max_episode_steps = 20\n",
        "\n",
        "cartpole_model = PPO(MlpPolicy, cartpole_env, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYasdZXR39zE"
      },
      "source": [
        "We import a helper function to evaluate the agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Rr0B_j4AJ1"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFkdVC6T4MT9"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FJG3toK4NI3"
      },
      "source": [
        "# Use a separate environment for evaluation\n",
        "cartpole_eval_env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(cartpole_model, cartpole_eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"Before training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE0DszDd4q3T"
      },
      "source": [
        "Train the agent and save it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5AZRqQw4vSA"
      },
      "source": [
        "# Train the agent for 10000 steps\n",
        "cartpole_model.learn(total_timesteps=10000)\n",
        "cartpole_model.save(\"ppo_cartpole\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uxE2zX319G6"
      },
      "source": [
        "Since we saved the model, we can delete it from memory and reload it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNBVdFjycBGz"
      },
      "source": [
        "# Since we saved the model, we can delete it from memory and reload it\n",
        "del cartpole_model\n",
        "\n",
        "cartpole_model = PPO.load(\"ppo_cartpole\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTJ65Oiv2Aza"
      },
      "source": [
        "Evaluate the trained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQozcy4E427F",
        "outputId": "27a75baf-45b5-40e6-da25-2522a6a7e037"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(cartpole_model, cartpole_eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"After training: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "After training: mean_reward:197.49 +/- 10.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtAWjf8u4__z"
      },
      "source": [
        "### Task 12 (2 points)\n",
        "\n",
        "How do you know if the training went well or not? _(This is a written answer question)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gLhRrQR5geU"
      },
      "source": [
        "_Please type your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WwzkWjX6LJt"
      },
      "source": [
        "**Another piece of helper code to visualize the trained environment**\n",
        "\n",
        "In this implementation we do not need to use `wrap_env` as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17-9WSVZ6PvU"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def render_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyFQ7MCI-1Ni"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0SmJkeD-2Ql"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG_Z5yVn2WPg"
      },
      "source": [
        "Record a video for the Cartpole env we trained earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YPviuRC_B3s"
      },
      "source": [
        "record_video('CartPole-v0', cartpole_model, video_length=500, prefix='ppo-cartpole')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GhrgssF2g8V"
      },
      "source": [
        "Render the generated video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0aaa_Iy_Tgn"
      },
      "source": [
        "render_videos('videos', prefix='ppo-cartpole-step-0-to-step-500')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB1atl7MKqpB"
      },
      "source": [
        "### Using Pre-trained RL Agents\n",
        "\n",
        "You can train other models such as the MountainCar using the available algorithms in the stable-baselines3 library. However, to train these models, it might take a lot of time. \n",
        "\n",
        "You may clone the [rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) repository, install all the required libraries, and train an agent to successfully complete the MountainCar problem. However, to train this agent, it may take a day or two!\n",
        "\n",
        "**Note:** If you are interested, you can use multi-processing capabilities in the stable-baselines3 library to speed up the process as outlined in this [online guide](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/multiprocessing_rl.ipynb). (Please note that this guide was written for the previous version of stable-baselines and you may need to adjust the libraries as needed). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j96ym-1KxKS"
      },
      "source": [
        "## Note: Only run this if you can wait for several hours (or days!) for the training process to complete.\n",
        "# !git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "# cd /content/rl-baselines3-zoo/\n",
        "# !pip install -r requirements.txt\n",
        "# !python train.py --algo ppo --env MountainCar-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 2 --sampler tpe --pruner median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWmPqVuOjFI"
      },
      "source": [
        "So, instead we will make use of the [rl-trained-agents](https://github.com/DLR-RM/rl-trained-agents), and generate a video for viewing. Here we use the pre-trained [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) model to the `MountainCar-v0` and generate a video with 50,000 timesteps.\n",
        "\n",
        "Let's first clone the repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h7McVfnOuWb"
      },
      "source": [
        "!git clone --recursive https://github.com/DLR-RM/rl-trained-agents.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r83O0Ces2_7v"
      },
      "source": [
        "Then, load the pre-trained model, record the video, render it, and display the mean reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "309Iqge9PWi0"
      },
      "source": [
        "from stable_baselines3 import DQN\n",
        "import gym\n",
        "\n",
        "env_name = \"MountainCar-v0\"\n",
        "\n",
        "mountaincar_dqn_model = DQN.load(\"rl-trained-agents/dqn/MountainCar-v0_1/MountainCar-v0.zip\")\n",
        "\n",
        "record_video(env_name, mountaincar_dqn_model, video_length=10000, prefix='dqn_mountaincar')\n",
        "\n",
        "render_videos('videos', prefix='dqn')\n",
        "\n",
        "mountaincar_eval_env = gym.make(env_name)\n",
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(mountaincar_dqn_model, mountaincar_eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ycf8dg7xMr"
      },
      "source": [
        "### Task 13 (6 points)\n",
        "\n",
        "Use two pre-trained models for the `Pendulum-v0` environment available at [rl-trained-agents](https://github.com/DLR-RM/rl-trained-agents), and generate the videos. \n",
        "Note: You may need to adjust the video_length in the videos to an optimal value for comparions. (4 points)\n",
        "\n",
        "Based on the videos generated, what can you say about the performance of the two models? (2 points) _(This is a written answer question.)_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6ucGuT2wBTd"
      },
      "source": [
        "# Type your code here for model 1 (2 points).\n",
        "\n",
        "\n",
        "# Type your code here for model 2 (2 points)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SengQl9Vm2qi"
      },
      "source": [
        "_Type your answer to the written answer question here (2 points)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ESRA0Sxw-rj"
      },
      "source": [
        "## Deep-RL with Atari Games\n",
        "\n",
        "For this example, we will use Lunar Lander environment.\n",
        "\n",
        "\"Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine. \"\n",
        "\n",
        "Lunar Lander environment: [https://gym.openai.com/envs/LunarLander-v2/](https://gym.openai.com/envs/LunarLander-v2/)\n",
        "\n",
        "![Lunar Lander](https://cdn-images-1.medium.com/max/960/1*f4VZPKOI0PYNWiwt0la0Rg.gif)\n",
        "\n",
        "We will apply the same process as before to load a pre-trained model and view the Lunar Lander in action. In this code snippet, we are using the [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vypvySbvoOb6"
      },
      "source": [
        "from stable_baselines3 import A2C\n",
        "import gym\n",
        "\n",
        "env_name = \"LunarLander-v2\"\n",
        "\n",
        "lunarlander_a2c_model = A2C.load(\"rl-trained-agents/a2c/LunarLander-v2_1/LunarLander-v2.zip\")\n",
        "\n",
        "record_video(env_name, lunarlander_a2c_model, video_length=5000, prefix='a2c_lunarlander')\n",
        "\n",
        "render_videos('videos', prefix='a2c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMLRtMK1qnso"
      },
      "source": [
        "# Creating Your Own Gym Environment\n",
        "\n",
        "So far you have seen using existing OpenAI Gym environments. Let's create a simple environment called `BasicEnv`. There are two actions in this environment, first gets a reward of 1, second gets a reward of -1. if we take an action, we are in `state 1`, and depending on the action we receive the appropriate reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-FsDuyGjf0N"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "\n",
        "class BasicEnv(gym.Env):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''There are two actions, first gets a reward of 1, \n",
        "        second gets a reward of -1. '''\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "        self.observation_space = gym.spaces.Discrete(2)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # if we took an action, we were in state 1\n",
        "        state = 1\n",
        "    \n",
        "        if action == 2:\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = -1\n",
        "            \n",
        "        # regardless of the action, game is done after a single step\n",
        "        done = True\n",
        "\n",
        "        info = {}\n",
        "\n",
        "        return state, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        state = 0\n",
        "        return state\n",
        "\n",
        "    def render(self):\n",
        "        pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byJ3u68T3fXR"
      },
      "source": [
        "Create an instance of the `BasicEnv` and inspect the action and the observation space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o7M08bVkMwR"
      },
      "source": [
        "my_basic_env = BasicEnv()\n",
        "action_space_size = my_basic_env.action_space\n",
        "state_space_size = my_basic_env.observation_space\n",
        "print(action_space_size)\n",
        "print(state_space_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkjsCHj83nww"
      },
      "source": [
        "The stable_baselines3 library comes with a function that verifies if your environment is Gym-compatible.\n",
        "\n",
        "If the environment is defined properly, the function will not return anything. Which is a bit weird, but it means that everything is OK.\n",
        "\n",
        "You can test what happens if you change any of the elements in your environment. For example, if you don’t have a reset function, you will get a `NotImplementedError`. If you don’t have a proper `self`.`action_space` and `self.observation_space`, for example if you defined them as a regular list instead of the special `gym.spaces` class, you will get this error: `AssertionError: The action space must inherit from gym.spaces`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne_YCN_Xs9w5",
        "outputId": "cb028484-ddca-49c5-93ad-14ef3e1da59e"
      },
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "print(check_env(my_basic_env))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7US_hgd4Ynw"
      },
      "source": [
        "Similar to the earlier pre-defined gym environments, we can apply an RL algorithm implemented in stable_baselines3 to define an agent and train it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSkkxYBkuxxg"
      },
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "my_basic_model = PPO(\"MlpPolicy\", my_basic_env, verbose=1)\n",
        "my_basic_model.learn(total_timesteps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1UKpJ4N4tL5"
      },
      "source": [
        "After the model has been trained, we can run the environment for several episodes (in this case 10), and see the output in each episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu7iMfYCvGxD"
      },
      "source": [
        "obs = my_basic_env.reset()\n",
        "for i in range(10):\n",
        "    action, _states = my_basic_model.predict(obs)\n",
        "    obs, reward, done, info = my_basic_env.step(action)\n",
        "    print(action, obs, reward, done)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWwh-yRh5LdG"
      },
      "source": [
        "As you can see, the above environment is a bit boring because, regardless of the action, the game is done in one step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djJfMkNA1REo"
      },
      "source": [
        "### Task 14 (5 points)\n",
        "\n",
        "Develop a custom environment, titled `CustomEnv` with 3 actions. Give a reward for each action in a probabilitstic manner. For example, you may use a normal distribution based on the action provided to determine the reward from one of the three options [1, 0, -1]. (2 points)\n",
        "\n",
        "Use the environment checker to ascertain that the environment is defined correctly. (1 point)\n",
        "\n",
        "Use any `stable_baselines3` algorithm to train a model for the `CustomEnv` for 10,000 timesteps and display the output for 100 episodes. (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1OaUFGx7jlR"
      },
      "source": [
        "# Type your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}